import os
import sys
import re
import logging
import warnings
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.stats import skew, kurtosis, shapiro, kstest, norm
from sklearn.decomposition import PCA
from io import StringIO

# Disable interactive mode for plotting
plt.ioff()  # Disable interactive mode

# Example GUI class implementation for demonstration purposes
class GUI:
    def display(self, content):
        # Implement the method to display content in your GUI
        print(content)  # For example purposes, it just prints the content

# Initialize the GUI object
gui = GUI()

########### BULK FACE-DISTANCE STATISTICS #################
# A script developed by klromans557 (a.k.a reaper557) with
# the help of GPT-4o. Thanks Zedd! (>'.')>[<3] [7/25/2024]
#
# Version 2.1 [7/29/2024]
#
# The purpose of this script is to perform a statistical
# analysis of the data generated by the create_facedist 
# script. 
# A attempt is made at developing an "objective"
# score, called the General Accuracy Score (GAS)
# in which to test similar models for their ability to 
# replicate the likeness of the commonly trained subject.
# In other words, I was getting tired of "eyeballing" model
# output images and trying to figure out which one was doing
# a better job, and I wanted to quantify and automate this
# process. I also wanted to get a better grasp on how
# changes to training methodology, e.g. hyperparamter sett.,
# image quality in the training/REG data sets, and captions
# in the REG set (question that inspired me here) can affect
# a model's ability to replicate the subject's likeness.  
############################################################

# ===== USER DEFINED VARIABLES =============================
# ¡NOTE! DO NOT CHANGE these unless YOU know what you are doing <(O.O)> 
# ¡NOTE! GPT-4o and I optimized for this order, and the associated weights, through tremendous back-n-forth, significant error-correction, and testing a bunch of alternatives
# ¡NOTE! What I had before was not very "thought out", whereas this new set of parameters has been considered carefully and significant redundencies removed through correlation analysis
# ¡NOTE! All metrics either already have, or will be scaled to have, the same units as the data. The above optimized weights took that into account; see 'scale_factor_metrics' below. 
order_of_import_of_metrics = ["Median", "P90", "Sc_Skewness", "Sc_Kurtosis", "IQR", "SD"] 
# Optimized for above fixed order of import using script in '_EXTRAS' and example data with good statistics (i.e. many data points)
# Optimized weights seem to favor the central bulk parameters, i.e. Median & IQR
optimized_weights = np.array([0.31200408, 0.05169795, 0.01367902, 0.07882819, 0.35433335, 0.18945741]) 
# 
# My so called "Meat-N-Potatoes" weights which are a SANITY CHECK, of sorts
# That the most important metrics are always weighted highly, i.e. Median & P90
# With the maxim, 'more overall small-valued data, the better'
mnp_weights = np.array([0.5, 0.5, 0.0, 0.0, 0.0, 0.0]) 
# ===== USER DEFINED VARIABLES =============================

# ===== FIXED GLOBALS ======================================
# Thresholds to switch between normality tests and
# for determining result from normality tests
sample_size_threshold = 5000 # S-W test cannot handle very large sample sizes like K-S can, but is quick otherwise
sw_threshold = 0.95 # "Shapiro-Wilk"
ks_threshold = 0.05 # "Kolmogorov-Smirnov"
pvalue_threshold = 0.05 # For testing the NULL-hypothesis

# DEBUG MSG ================================================
#print(f"Metric log file path: {metric_log_file}")
#print(f"Output stats file path: {output_stats_file}")
# ===== FIXED GLOBALS ======================================

# ===== LOGGING ============================================
# Logger configuration
# Setup logging function with verbosity levels and real-time flush
LOG_LEVEL = "INFO"  # Change to DEBUG, INFO, WARNING, ERROR, or CRITICAL as needed

def setup_logging():
    """
    Sets up two separate loggers: metrics_logger and output_logger.
    Clears any existing global logger handlers to prevent interference.
    """
    log_dir = os.path.join(os.getcwd(), 'LOGS')
    os.makedirs(log_dir, exist_ok=True)

    # Safeguard: Clear existing handlers on the root logger
    logging.getLogger().handlers.clear()

    # Create loggers
    loggers = {}

    # Metric log file logger
    metrics_logger = logging.getLogger('metrics_logger')
    metrics_logger.setLevel(getattr(logging, LOG_LEVEL.upper(), logging.INFO))
    metric_file_handler = logging.FileHandler(os.path.join(log_dir, 'metric_weight_normal_stats.txt'))
    metric_file_handler.setFormatter(logging.Formatter('%(message)s'))
    metrics_logger.addHandler(metric_file_handler)
    loggers['metric_weight_normal_stats.txt'] = metrics_logger

    # Output stats log file logger
    output_logger = logging.getLogger('output_logger')
    output_logger.setLevel(getattr(logging, LOG_LEVEL.upper(), logging.INFO))
    output_file_handler = logging.FileHandler(os.path.join(log_dir, 'output_stats.txt'))
    output_file_handler.setFormatter(logging.Formatter('%(message)s'))
    output_logger.addHandler(output_file_handler)
    loggers['output_stats.txt'] = output_logger

    return loggers

def log(message, level="INFO", file_target=None):
    """
    Routes log messages to the appropriate logger based on the file_target.
    Logs to the console and the specified logger.
    """
    if not file_target:
        raise ValueError("file_target must be specified.")

    # Extract the logger based on the file_target
    logger_name = os.path.basename(file_target)
    logger = loggers.get(logger_name)

    if not logger:
        raise ValueError(f"No logger configured for file target: {file_target}")

    # Log the message with the specified level
    getattr(logger, level.lower(), logger.info)(message)

    # Automatically update GUI if logging to output_stats.txt
    if file_target.endswith("output_stats.txt") and hasattr(gui, 'display'):
        gui.display(message)

def log_debug(message, file_target="LOGS/metric_weight_normal_stats.txt"):
    """
    Logs debug messages if the LOG_LEVEL is set to DEBUG.
    Routes to the correct logger based on the file_target.
    """
    if LOG_LEVEL == "DEBUG":
        log(message, level="DEBUG", file_target=file_target)

# ===== LOGGING ============================================

# ===== FUNCTION ZOO =======================================
def check_output_integrity(output_dir):
    try:
        valid = True
        for subdir, _, files in os.walk(output_dir):
            for file in files:
                if file.endswith(".txt"):
                    filepath = os.path.join(subdir, file)
                    try:
                        with open(filepath, 'r') as f:
                            data = eval(f.read())
                            if not isinstance(data, list) or not all(isinstance(x, (float, int)) for x in data):
                                log(f"Corrupted file detected: {filepath}", level="ERROR", file_target=metric_log_file)
                                valid = False
                    except Exception as e:
                        log(f"Error reading file {filepath}: {e}", level="ERROR", file_target=metric_log_file)
                        log_debug(traceback.format_exc())
                        valid = False
        return valid
    except Exception as e:
        log(f"Unexpected error during output integrity check: {e}", level="CRITICAL", file_target=metric_log_file)
        log_debug(traceback.format_exc())
        return False

def alphanumeric_sort_key(s):
    return [(int(text) if text.isdigit() else text.lower()) for text in re.split('([0-9]+)', s)]

def read_data_from_directory(directory):
    all_data = []
    try:
        for filename in os.listdir(directory):
            if filename.endswith(".txt"):
                filepath = os.path.join(directory, filename)
                with open(filepath, 'r') as file:
                    try:
                        data = eval(file.read())
                        if not isinstance(data, list):
                            raise ValueError(f"Invalid format in {filename}. Expected a list.")
                        cleaned_data = [float(value) for value in data if 0 <= float(value) <= 1]
                        all_data.extend(cleaned_data)
                    except Exception as e:
                        log(f"Skipping corrupted file {filepath}: {e}", level="WARNING", file_target=metric_log_file)
                        log_debug(traceback.format_exc())
    except Exception as e:
        log(f"Error reading directory {directory}: {e}", level="ERROR", file_target=metric_log_file)
        log_debug(traceback.format_exc())
    return np.array(all_data)

def get_metric_values(metrics, order):
    metric_names = order_of_import_of_metrics
    metric_dict = dict(zip(metric_names, metrics))
    ordered_metrics = [metric_dict[name] for name in order]
    return ordered_metrics

def calculate_metrics(values): 
    if len(values) == 0:
        return [np.nan] * len(order_of_import_of_metrics)
    
    # These metrics already have the same "units" as the data and do not need further preprocessing
    iqr = np.percentile(values, 75) - np.percentile(values, 25)
    perc_90 = np.percentile(values, 90)
    sd = np.std(values)
    median = np.median(values)
    
    # These metrics need a carefully chosen scale factor, as they are normally unitless, such that:
    #   i. Make kurtosis and skewness more comparable in magnitude to the other metrics as they were way too small (still small, but now less so!)
    #  ii. Choose a scale factor which focuses on the central tendency and spread information, i.e. median and iqr, contained in the bulk of the data
    #        while leaving the kurtosis and skewness in their crucial roles for characterizing distribution shapes and signifcant outliers
    # iii. Maintains that all metrics have the same "units" as the data itself, ensuring combinations/comparisons remain meaningful
    #  iv. Uses an overall "standardization" factor to throttle the scale and keep preference in the Median/P90 metrics;
    #      Also serves, as an attempt, to "normalize" the scaled metrics to the expected order of magnitude of the data, i.e. ~0.1. 
    scale_factor_metrics = 0.1 * np.power(median, 2)/iqr
    #print(scale_factor_metrics)
    kurtosis_value = kurtosis(values) * scale_factor_metrics
    skewness = skew(values) * scale_factor_metrics 

    #print(np.power(median, 2)/sd)
    #print(kurtosis_value)
    #print(skewness)

    metric_dict = {
        "IQR": iqr,
        "SD": sd,
        "Median": median,
        "Sc_Kurtosis": kurtosis_value,
        "P90": perc_90,
        "Sc_Skewness": skewness,
    }

    ordered_metrics = [metric_dict[metric] for metric in order_of_import_of_metrics]
    
    # All returned metrics are intended to have the same units as the data, and thus more "comparable"
    return ordered_metrics

# Adjust this to display more digits in logs/gui
def round_and_format(value, decimals=4, threshold=1e-4):
    if abs(value) < threshold:
        return f"{value:.{decimals}e}"  # Scientific notation with 'decimals' significant figures
    else:
        return f"{value:.{decimals}f}"  # Fixed-point notation with 'decimals' decimal places

def format_weight_list(weight_list, decimals=4, threshold=1e-4):
    # Format a list of weights by rounding using `round_and_format`.
    return [round_and_format(weight, decimals, threshold) for weight in weight_list]

def compute_bin_size(numbers):
    bins = int(np.ceil(np.log2(len(numbers)) + 1))
    return bins

def create_histogram_and_display_metrics(numbers1, numbers2, label1, label2, metrics1, metrics2):
    ordered_metrics1 = get_metric_values(metrics1, order_of_import_of_metrics)
    ordered_metrics2 = get_metric_values(metrics2, order_of_import_of_metrics)
    
    label1_metrics = ', '.join([f'{name}: {value:.4f}' for name, value in zip(order_of_import_of_metrics, ordered_metrics1)])
    label2_metrics = ', '.join([f'{name}: {value:.4f}' for name, value in zip(order_of_import_of_metrics, ordered_metrics2)])
    
    fig, axs = plt.subplots(2, 1, figsize=(10, 10))  # Fixed figure size
    
    # Calculate common bins
    all_data = np.concatenate([numbers1, numbers2])
    bin_edges = np.histogram_bin_edges(all_data, bins=compute_bin_size(all_data))
    
    # Histograms form the easiest and fastest way to make a determination
    axs[0].hist(numbers1, bins=bin_edges, alpha=0.5, 
                label=f'{label1} - {label1_metrics}', 
                edgecolor='black')
    axs[0].hist(numbers2, bins=bin_edges, alpha=0.5, 
                label=f'{label2} - {label2_metrics}', 
                edgecolor='black')
    axs[0].set_xlabel('Value')
    axs[0].set_ylabel('Frequency')
    axs[0].legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=1)
    axs[0].grid(True)
    
    # Q-Q plots give a quick way to see, at a glance, if a distribution is Gaussian (normal); Gaussian distributions form straight lines
    qq1 = stats.probplot(numbers1, dist="norm", plot=axs[1])
    line1 = axs[1].get_lines()[1]
    line1.set_color('green')
    line1.set_linestyle('--')
    scatter1 = axs[1].get_lines()[0]
    scatter1.set_color('blue')
    scatter1.set_markersize(scatter1.get_markersize() * 0.5)
    scatter1.set_label(f'{label1} Data')
    line1.set_label(f'{label1} Trendline')

    qq2 = stats.probplot(numbers2, dist="norm", plot=axs[1])
    line2 = axs[1].get_lines()[3]
    line2.set_color('red')
    line2.set_linestyle('--')
    scatter2 = axs[1].get_lines()[2]
    scatter2.set_color('orange')
    scatter2.set_markersize(scatter2.get_markersize() * 0.5)
    scatter2.set_label(f'{label2} Data')
    line2.set_label(f'{label2} Trendline')
    
    common_markersize = 20
    start_point1 = [line1.get_xdata()[0], line1.get_ydata()[0]]
    end_point1 = [line1.get_xdata()[-1], line1.get_ydata()[-1]]
    axs[1].plot(start_point1[0], start_point1[1], 'gx', markersize=common_markersize)
    axs[1].plot(end_point1[0], end_point1[1], 'gx', markersize=common_markersize)

    # Start/End points mostly to help see trendline even when data obscures it
    start_point2 = [line2.get_xdata()[0], line2.get_ydata()[0]]
    end_point2 = [line2.get_xdata()[-1], line2.get_ydata()[-1]]
    axs[1].plot(start_point2[0], start_point2[1], 'r+', markersize=common_markersize)
    axs[1].plot(end_point2[0], end_point2[1], 'r+', markersize=common_markersize)

    axs[1].set_title('')
    
    # The slope values 0f the Q-Q trendlines can help give a sense of "spread", but take it with a grain of salt
    slope1 = (end_point1[1] - start_point1[1]) / (end_point1[0] - start_point1[0])
    slope2 = (end_point2[1] - start_point2[1]) / (end_point2[0] - start_point2[0])

    # "Baby's First" automatic y-axis scale
    scale_factor = 0.2
    y_scale_start = (1 - scale_factor) * start_point1[1] if (start_point2[1] > start_point1[1]) else (1 - scale_factor) * start_point2[1]
    y_scale_end = (1 + scale_factor) * end_point2[1] if (end_point2[1] > end_point1[1]) else (1 + scale_factor) * end_point1[1]
    
    # "Uh-Oh" backup in case the above messes up
    if y_scale_end > 1:
        y_scale_end = 1
    if y_scale_start < 0:
        yscale_start = 0

    bbox_props = dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white", alpha=0.7)
    axs[1].text(0.95, 0.1, f'{label1} Slope: {slope1:.4f}', horizontalalignment='right', verticalalignment='center', transform=axs[1].transAxes, color='green', bbox=bbox_props)
    axs[1].text(0.95, 0.05, f'{label2} Slope: {slope2:.4f}', horizontalalignment='right', verticalalignment='center', transform=axs[1].transAxes, color='red', bbox=bbox_props)

    axs[1].legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)
    axs[1].grid(True)
    axs[1].set_ylim(y_scale_start, y_scale_end)

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    
    plot_filename = os.path.join(os.getcwd(), '_EXTRAS', 'comparison_plot.png')
    plt.savefig(plot_filename)
    #print(f"Comparison plot saved to: {plot_filename}")
    plt.close(fig)  # Close the figure to avoid displaying it; the was a "JANK" solution to a silly gui issue

# Suppress specific sklearn warnings
warnings.filterwarnings("ignore", message="The covariance matrix associated to your dataset is not full rank")

# Preprocess Function to Remove Outliers
# def preprocess_for_robust_pca(metrics):
    # outlier_detector = EllipticEnvelope(contamination=0.1)  # Adjust contamination as needed
    # is_inlier = outlier_detector.fit_predict(metrics)
    # cleaned_metrics = metrics[is_inlier == 1]
    # return cleaned_metrics

def preprocess_for_robust_pca(metrics):
    metrics = np.array(metrics)
    
    # Calculate the first and third quartiles (Q1 and Q3) for each feature
    Q1 = np.percentile(metrics, 25, axis=0)
    Q3 = np.percentile(metrics, 75, axis=0)
    
    # Calculate the IQR (Interquartile Range) for each feature
    IQR = Q3 - Q1
    
    # Define outlier thresholds (1.5 * IQR is a common choice)
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Identify inliers: metrics that are within the bounds for all features
    is_inlier = np.all((metrics >= lower_bound) & (metrics <= upper_bound), axis=1)
    
    # Filter out the outliers
    cleaned_metrics = metrics[is_inlier]
    
    return cleaned_metrics

# Standard PCA method was being messed up by noisey distributions, this tends to work better
def calculate_robust_pca_weights(metrics):
    metrics = np.array(metrics)
    # Preprocess the metrics to remove outliers
    cleaned_metrics = preprocess_for_robust_pca(metrics)
    
    # Check if cleaned_metrics has enough samples
    if cleaned_metrics.shape[0] < 2:
        #print("Robust PCA calculation error: Not enough samples after outlier removal.")
        return np.array([0.5, 0.0, 0.0, 0.0, 0.5, 0.0])  # Default to the "Meat and Potatoes", the central bulk's position and spread (i.e. Mean & IQR)

    # Determine the number of components to use
    n_components = min(cleaned_metrics.shape[0], cleaned_metrics.shape[1])
    
    # Suppress covariance matrix warnings
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", UserWarning)
        # Perform PCA on the cleaned data
        pca = PCA(n_components=n_components)
        pca.fit(cleaned_metrics)
    
    # Sum of absolute values of loadings in the PCA components to determine importance
    weights = np.sum(np.abs(pca.components_), axis=0)
    weights /= np.sum(weights)  # Normalize weights
    return weights

def calculate_weighted_rank_sum_weights(order_of_importance, optimized_weights, alpha=0.5):
    n = len(order_of_importance)
    
    # Assign ranks based on the order of importance
    ranks = np.arange(1, n + 1)
    
    # Calculate rank-based weights
    rank_based_weights = (n - ranks + 1) / np.sum(n - ranks + 1)
    
    # Convert to NumPy arrays to ensure proper calculations
    rank_based_weights = np.array(rank_based_weights, dtype=float)
    optimized_weights = np.array(optimized_weights, dtype=float)
    
    # Combine rank-based weights with optimized weights
    final_weights = alpha * rank_based_weights + (1 - alpha) * optimized_weights
    
    return final_weights

def calculate_inverse_variance_weights(metrics):
    variances = np.var(metrics, axis=0)
    inverse_variances = 1 / variances
    weights = inverse_variances / np.sum(inverse_variances)
    return weights

def calculate_ahp_weights(num_metrics):
    comparison_matrix = np.ones((num_metrics, num_metrics))
    for i in range(num_metrics):
        for j in range(i + 1, num_metrics):
            comparison_matrix[i, j] = j - i
            comparison_matrix[j, i] = 1 / comparison_matrix[i, j]

    column_sums = np.sum(comparison_matrix, axis=0)
    normalized_matrix = comparison_matrix / column_sums

    weights = np.mean(normalized_matrix, axis=1)
    weights = weights / np.sum(weights)  # Ensure weights sum to 1
    return weights

# Early "normality" test which attempts to compare each model distribution with corresponding synthetic normal with same Mean/Median and SD 
def generate_synthetic_normal_metrics(mean, sd, size):
    if sd <= 0:
        sd = 0.075
    synthetic_normal = np.random.normal(mean, sd, size)
    return calculate_metrics(synthetic_normal)

def calculate_fractional_percent_difference(data_metrics, synthetic_metrics):
    return [abs((d - s) / s) * 100 for d, s in zip(data_metrics, synthetic_metrics)]

def insert_newlines(text, line_length):
    lines = []
    while len(text) > line_length:
        split_pos = text[:line_length].rfind(' ')
        if split_pos == -1:
            split_pos = line_length
        lines.append(text[:split_pos])
        text = text[split_pos:].strip()
    lines.append(text)
    return '\n'.join(lines)

# Using S-W and K-S normality tests depending on sample size
def perform_normality_test(values):
    if len(values) <= sample_size_threshold:
        test_result = shapiro(values)
        test_name = "Shapiro-Wilk"
    else:
        norm_samples = norm.rvs(size=len(values))
        test_result = kstest(values, 'norm', args=(np.mean(values), np.std(values)))
        test_name = "Kolmogorov-Smirnov"
    return test_result, test_name

def interpret_test_results(test_result, test_name):
    if test_name == "Shapiro-Wilk":
        W = test_result.statistic
        p_value = test_result.pvalue
        if p_value > pvalue_threshold and W > sw_threshold:
            normal_status = "Normal Status: EXPECTED!"
        elif p_value <= pvalue_threshold and W < sw_threshold:
            normal_status = "Normal Status: NOT expected!"
        else:
            normal_status = "Normal Status: ???"
    elif test_name == "Kolmogorov-Smirnov":
        D = test_result.statistic
        p_value = test_result.pvalue
        if p_value > pvalue_threshold and D <= ks_threshold:
            normal_status = "Normal Status: EXPECTED!"
        elif p_value <= pvalue_threshold and D > ks_threshold:
            normal_status = "Normal Status: NOT expected!"
        else:
            normal_status = "Normal Status: ???"
    return normal_status

# The "Heart" which does the main direct Two-Model Direct Comparisons
def perform_comparison(dir_model_1, dir_model_2):
    values_1 = read_data_from_directory(dir_model_1)
    values_2 = read_data_from_directory(dir_model_2)
    
    folder_name_1 = os.path.basename(dir_model_1)
    folder_name_2 = os.path.basename(dir_model_2)

    if values_1.size == 0 or values_2.size == 0:
        print(f"Skipping comparison between {folder_name_1} and {folder_name_2} due to empty data.")
        return [np.nan] * len(order_of_import_of_metrics), [np.nan] * len(order_of_import_of_metrics), [], [], [], [], [], [], []

    metrics_1 = calculate_metrics(values_1)
    metrics_2 = calculate_metrics(values_2)
    
    combined_metrics = np.array([metrics_1, metrics_2])

    uniform_weights = np.ones(combined_metrics.shape[1]) / combined_metrics.shape[1]
    opt = optimized_weights  # Assuming optimized weights for simplicity
    
    # Use Weighted Rank Sum method
    weighted_rank_sum_weights = calculate_weighted_rank_sum_weights(order_of_import_of_metrics, opt)
    
    # Inverse-Variance Method
    inverse_variance_weights = calculate_inverse_variance_weights(combined_metrics)
    
    # Analytic Hierarchy Process Method
    ahp_weights = calculate_ahp_weights(len(order_of_import_of_metrics))
    
    # Robust Principal Component Analysis
    try:
        robust_pca_weights = calculate_robust_pca_weights(combined_metrics)  # Calculate Robust PCA weights
    except ValueError as e:
        print(f"Robust PCA calculation error: {e}")
        robust_pca_weights = np.nan * np.ones(len(order_of_import_of_metrics))  # Handle the error by assigning NaN weights

    def calculate_scores(weights):
        general_accuracy_score_1 = np.dot(metrics_1, weights)
        general_accuracy_score_2 = np.dot(metrics_2, weights)
        #print(f"GAS 1: {general_accuracy_score_1}")
        #print(f"GAS 2: {general_accuracy_score_2}")
        return general_accuracy_score_1, general_accuracy_score_2

    uniform_score_1, uniform_score_2 = calculate_scores(uniform_weights)
    optimized_score_1, optimized_score_2 = calculate_scores(optimized_weights)
    weighted_rank_sum_score_1, weighted_rank_sum_score_2 = calculate_scores(weighted_rank_sum_weights)
    inverse_variance_score_1, inverse_variance_score_2 = calculate_scores(inverse_variance_weights)
    ahp_score_1, ahp_score_2 = calculate_scores(ahp_weights)
    robust_pca_score_1, robust_pca_score_2 = calculate_scores(robust_pca_weights)
    mnp_score_1, mnp_score_2 = calculate_scores(mnp_weights)


    average_score_1 = np.mean([uniform_score_1, optimized_score_1, weighted_rank_sum_score_1, inverse_variance_score_1, ahp_score_1, robust_pca_score_1, mnp_score_1])
    average_score_2 = np.mean([uniform_score_2, optimized_score_2, weighted_rank_sum_score_2, inverse_variance_score_2, ahp_score_2, robust_pca_score_2, mnp_score_2])

    return average_score_1, average_score_2, uniform_weights, optimized_weights, weighted_rank_sum_weights, inverse_variance_weights, ahp_weights, robust_pca_weights, mnp_weights

# Only top winners of Round-Robin are reported
def report_top_winners(win_counts, top_n=3, logger=None, model_dirs=[]):
    """
    Reports the top winners in a round-robin comparison.

    Parameters:
    win_counts (dict): A dictionary of model indices and their respective win counts.
    top_n (int): Number of top winners to report.
    logger (function): The logging function (e.g., log).
    model_dirs (list): List of model directories.

    Returns:
    int: The index of the top winner.
    """
    sorted_win_counts = sorted(win_counts.items(), key=lambda item: item[1], reverse=True)
    if logger:
        logger("Top Winners:", file_target="LOGS/output_stats.txt")
        for i, (model_index, count) in enumerate(sorted_win_counts[:top_n]):
            folder_name = os.path.basename(model_dirs[model_index])
            logger(f"Top {i + 1}: {folder_name} with {count} wins", file_target="LOGS/output_stats.txt")
    return sorted_win_counts[0][0]  # Return the index of the top winner

# Defines the Round-Robin tournament style
def round_robin_comparisons(model_dirs):
    num_models = len(model_dirs)
    comparison_results = {i: [] for i in range(num_models)}
    win_counts = {i: 0 for i in range(num_models)}

    for i in range(num_models):
        for j in range(i + 1, num_models):
            agas_1, agas_2 = perform_comparison(model_dirs[i], model_dirs[j])[:2]
            agas_1 = np.array(agas_1)
            agas_2 = np.array(agas_2)
            
            if not np.isnan(agas_1).any():
                comparison_results[i].append(agas_1)
            if not np.isnan(agas_2).any():
                comparison_results[j].append(agas_2)

            if agas_1 < agas_2:
                win_counts[i] += 1
            elif agas_2 < agas_1:
                win_counts[j] += 1

    return comparison_results, win_counts

# ===== FUNCTION ZOO =======================================

# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [[[ MAIN ]]] XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
def main():
    # Automatically sets directories and files
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.getenv('OUTPUT_DIR', os.path.join(script_dir, 'DIR', 'output'))  # Directory containing model folders

    # Logging files and folder
    logs_dir = os.getenv('LOGS_DIR', os.path.join(script_dir, 'LOGS'))
    metric_log_file = os.path.join(logs_dir, 'metric_weight_normal_stats.txt')
    output_stats_file = os.path.join(logs_dir, 'output_stats.txt')

    # Clear the log files for a new run
    with open(metric_log_file, "w") as file:
        file.write("")  # Clear contents of metric log file
    with open(output_stats_file, "w") as file:
        file.write("")  # Clear contents of output stats file

    # Set up loggers
    global loggers
    loggers = setup_logging()

    # Example usage with the log() function
    log("Logging initialized at INFO level.", level="INFO", file_target=metric_log_file)
    log("Logging initialized for output statistics.", level="INFO", file_target=metric_log_file)

    # Well, is it?
    def is_directory_empty(directory):
        return not any(os.scandir(directory))

    # Check if the file exists
    if not os.path.exists(metric_log_file):
        # Create an empty file
        with open(metric_log_file, 'w') as file:
            pass  # Just creating an empty file

    if not os.path.exists(output_stats_file):
        # Create an empty file
        with open(output_stats_file, 'w') as file:
            pass  # Just creating an empty file

    # Ensure the LOGS directory exists
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
    log_debug(f"Logs directory ensured at: {logs_dir}", file_target=metric_log_file)
    log_debug(f"Metric log file created: {metric_log_file}", file_target=metric_log_file)
    log_debug(f"Output stats file created: {output_stats_file}", file_target=output_stats_file)
    
    models_to_compare_directly = os.getenv('MODELS_TO_COMPARE', "1,2").split(',')
    try:
        models_to_compare_directly = [int(model.strip()) for model in models_to_compare_directly]
    except ValueError:
        error_message = "models_to_compare_directly must contain comma-separated integer values."
        log(error_message, level="CRITICAL", file_target=output_stats_file)
        sys.exit(error_message)
        
    if is_directory_empty(base_dir):
        log("Error: The 'output' folder is empty.", level="CRITICAL", file_target=output_stats_file)
        return

    log_debug(f"Starting integrity check for base directory: {base_dir}")

    if not check_output_integrity(base_dir):
        log("Output integrity check failed. Please fix corrupted files and try again.", level="CRITICAL", file_target=metric_log_file)
        sys.exit(1)

    log_debug("Output directory passed integrity check.")

    model_dirs = [os.path.join(base_dir, subdir) for subdir in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, subdir))]
    model_dirs.sort()  # Sort the model directories by name

    # Check if directories already have prepended numbers
    if not all(os.path.basename(d).split('_', 1)[0].isdigit() for d in model_dirs):
        # Sort the model directories by the alphanumeric sort key
        model_dirs.sort(key=alphanumeric_sort_key)
        
        # Rename directories with prepended numbers for user clarity
        renamed_dirs = []
        for i, model_dir in enumerate(model_dirs, start=1):
            base_name = os.path.basename(model_dir)
            new_name = f"{i:02d}_{base_name}"  # Use two digits to ensure proper sorting
            new_path = os.path.join(base_dir, new_name)
            os.rename(model_dir, new_path)
            renamed_dirs.append(new_path)
        model_dirs = renamed_dirs  # Update model_dirs with renamed directories

    # Sort the model directories by the prepended number (now as integers)
    model_dirs.sort(key=lambda x: int(os.path.basename(x).split('_', 1)[0]))

    if len(model_dirs) < 2:
        log("Error: There must be at least two model directories to compare.", level="ERROR", file_target=output_stats_file)
        return

    if len(models_to_compare_directly) != 2:
        error_message = "models_to_compare_directly must contain exactly two numbers."
        log(error_message, level="CRITICAL", file_target=output_stats_file)
        sys.exit(error_message)

    model_idx_1, model_idx_2 = models_to_compare_directly
    if model_idx_1 <= 0 or model_idx_2 <= 0:
        error_message = "Model indices must be positive integers."
        log(error_message, level="CRITICAL", file_target=output_stats_file)
        sys.exit(error_message)

    if model_idx_1 > len(model_dirs) or model_idx_2 > len(model_dirs):
        error_message = f"Model indices must be less than or equal to the number of model directories ({len(model_dirs)})."
        log(error_message, level="CRITICAL", file_target=output_stats_file)
        sys.exit(error_message)

    # For Python indexing; first position is '0'
    model_idx_1 -= 1
    model_idx_2 -= 1

    try:
        values_1 = read_data_from_directory(model_dirs[model_idx_1])
        values_2 = read_data_from_directory(model_dirs[model_idx_2])
    except Exception as e:
        log(f"Error reading data from directory: {e}", level="ERROR", file_target=metric_log_file)
        log_debug(traceback.format_exc())

    log_debug(f"Loaded {len(values_1)} data points from {model_dirs[model_idx_1]}.")
    log_debug(f"Loaded {len(values_2)} data points from {model_dirs[model_idx_2]}.")

    folder_name_1 = os.path.basename(model_dirs[model_idx_1])
    folder_name_2 = os.path.basename(model_dirs[model_idx_2])

    if values_1.size == 0 or values_2.size == 0:
        print(f"Error: One or both specified model directories have no valid data.")
        return

    if not check_output_integrity(base_dir):
        print("Output integrity check failed. Please fix corrupted files and try again.")
        sys.exit(1)

    metrics_1 = calculate_metrics(values_1)
    metrics_2 = calculate_metrics(values_2)

    log_debug(f"Metrics for {folder_name_1}: {metrics_1}")
    log_debug(f"Metrics for {folder_name_2}: {metrics_2}")

    # Sythnetic normals to compare with each distributions; assuming mean = Mean & sd = IQR from data
    synthetic_metrics_1 = generate_synthetic_normal_metrics(metrics_1[0], metrics_1[4], len(values_1))
    synthetic_metrics_2 = generate_synthetic_normal_metrics(metrics_2[0], metrics_2[4], len(values_2))
    
    # Scale the appropriate synthetic metrics to match scaled data ones
    syth_factor_1 = 0.1 * np.power(synthetic_metrics_1[0], 2)/synthetic_metrics_1[4]
    syth_factor_2 = 0.1 * np.power(synthetic_metrics_2[0], 2)/synthetic_metrics_2[4]
    synthetic_metrics_1[2] *= syth_factor_1
    synthetic_metrics_2[2] *= syth_factor_2  
    synthetic_metrics_1[3] *= syth_factor_1
    synthetic_metrics_2[3] *= syth_factor_2

    fractional_percent_diff_1 = calculate_fractional_percent_difference(metrics_1, synthetic_metrics_1)
    fractional_percent_diff_2 = calculate_fractional_percent_difference(metrics_2, synthetic_metrics_2)
    
    normality_test_1, test_name_1 = perform_normality_test(values_1)
    normality_test_2, test_name_2 = perform_normality_test(values_2)
    
    normal_status_1 = interpret_test_results(normality_test_1, test_name_1)
    normal_status_2 = interpret_test_results(normality_test_2, test_name_2)

    # METRICS =========================
    # Start "Metric Values" section
    symbol_multiple = 25
    
    log("\n" + "=" * symbol_multiple, file_target=metric_log_file)
    log("Metric Values", file_target=metric_log_file)
    log("=" * symbol_multiple, file_target=metric_log_file)

    # Format and log metric values
    metric_dict_1 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(metrics_1, order_of_import_of_metrics))}
    metric_dict_2 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(metrics_2, order_of_import_of_metrics))}
    
    # Log metrics for each model
    log(f"Metrics for {folder_name_1}: {metric_dict_1}", level="INFO", file_target=metric_log_file)
    log(f"Metrics for {folder_name_2}: {metric_dict_2}", level="INFO", file_target=metric_log_file)

    # Create and display histograms of the data
    create_histogram_and_display_metrics(values_1, values_2, folder_name_1, folder_name_2, metrics_1, metrics_2)
    
    # Perform direct comparison
    average_score_1, average_score_2, uniform_weights, optimized_weights, weighted_rank_sum_weights, inverse_variance_weights, ahp_weights, robust_pca_weights, mnp_weights = perform_comparison(model_dirs[model_idx_1], model_dirs[model_idx_2])
    log_debug(f"Comparison results: {folder_name_1} GAS = {average_score_1}, {folder_name_2} GAS = {average_score_2}") 
    
    # Log weight values
    log("\n" + "=" * symbol_multiple, file_target=metric_log_file)
    log("Weight Values", file_target=metric_log_file)
    log("=" * symbol_multiple, file_target=metric_log_file)

    log(f"Uniform Weights: {format_weight_list(uniform_weights)}", file_target=metric_log_file)
    log(f"Optimized Weights: {format_weight_list(optimized_weights)}", file_target=metric_log_file)
    log(f"Weighted Rank Sum Weights: {format_weight_list(weighted_rank_sum_weights)}", file_target=metric_log_file)
    log(f"Inverse Variance Weights: {format_weight_list(inverse_variance_weights)}", file_target=metric_log_file)
    log(f"Analytic Hierarchy Process Weights: {format_weight_list(ahp_weights)}", file_target=metric_log_file)
    log(f"Robust PCA Weights: {format_weight_list(robust_pca_weights)}", file_target=metric_log_file)
    log(f"Meat-N-Potatoes Weights: {format_weight_list(mnp_weights)}", file_target=metric_log_file)

    synthetic_metric_dict_1 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(synthetic_metrics_1, order_of_import_of_metrics))}
    synthetic_metric_dict_2 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(synthetic_metrics_2, order_of_import_of_metrics))}

    # Normal Tests Header
    log("\n" + "=" * symbol_multiple, file_target=metric_log_file)
    log(f"Normal Tests", file_target=metric_log_file)
    log("=" * symbol_multiple, file_target=metric_log_file)

    # Synthetic Metrics
    log(f"Synthetic Metrics for {folder_name_1}: {synthetic_metric_dict_1}", file_target=metric_log_file)
    log(f"Synthetic Metrics for {folder_name_2}: {synthetic_metric_dict_2}", file_target=metric_log_file)

    # Percent Differences
    log("-" * symbol_multiple, file_target=metric_log_file)
    log(f"Synthetic Normal Distribution Metrics Comparison for {folder_name_1} (percent differences):", file_target=metric_log_file)
    for metric_name, percent_diff in zip(order_of_import_of_metrics, fractional_percent_diff_1):
        log(f"{metric_name}: {round_and_format(percent_diff)}%", file_target=metric_log_file)

    log("\n" + f"Synthetic Normal Distribution Metrics Comparison for {folder_name_2} (percent differences):", file_target=metric_log_file)
    for metric_name, percent_diff in zip(order_of_import_of_metrics, fractional_percent_diff_2):
        log(f"{metric_name}: {round_and_format(percent_diff)}%", file_target=metric_log_file)

    # Shapiro-Wilk Test Results for Model 1
    log("-" * symbol_multiple, file_target=metric_log_file)
    log(f"{test_name_1} Test for {folder_name_1}: statistic={normality_test_1.statistic:.5f}, p-value={normality_test_1.pvalue:.5f}", file_target=metric_log_file)
    log(f"Normal Status for {folder_name_1}: {normal_status_1}", file_target=metric_log_file)

    # Shapiro-Wilk Test Results for Model 2
    log("\n" + f"{test_name_2} Test for {folder_name_2}: statistic={normality_test_2.statistic:.5f}, p-value={normality_test_2.pvalue:.5f}", file_target=metric_log_file)
    log(f"Normal Status for {folder_name_2}: {normal_status_2}", file_target=metric_log_file)

    # Footer Separator
    log("\n" + "=" * symbol_multiple + "\n", file_target=metric_log_file)

    # METRICS =========================

    wins_1 = 0
    wins_2 = 0

    def calculate_scores(weights):
        general_accuracy_score_1 = np.dot(metrics_1, weights)
        general_accuracy_score_2 = np.dot(metrics_2, weights)
        return general_accuracy_score_1, general_accuracy_score_2

    uniform_score_1, uniform_score_2 = calculate_scores(uniform_weights)
    optimized_score_1, optimized_score_2 = calculate_scores(optimized_weights)
    weighted_rank_sum_score_1, weighted_rank_sum_score_2 = calculate_scores(weighted_rank_sum_weights)
    inverse_variance_score_1, inverse_variance_score_2 = calculate_scores(inverse_variance_weights)
    ahp_score_1, ahp_score_2 = calculate_scores(ahp_weights)
    robust_pca_score_1, robust_pca_score_2 = calculate_scores(robust_pca_weights)
    mnp_score_1, mnp_score_2 = calculate_scores(mnp_weights)

    if uniform_score_1 < uniform_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if optimized_score_1 < optimized_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if weighted_rank_sum_score_1 < weighted_rank_sum_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if inverse_variance_score_1 < inverse_variance_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if ahp_score_1 < ahp_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if robust_pca_score_1 < robust_pca_score_2:
        wins_1 += 1
    else:
        wins_2 += 1
    
    if mnp_score_1 < mnp_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    average_score_1 = np.mean([uniform_score_1, optimized_score_1, weighted_rank_sum_score_1, inverse_variance_score_1, ahp_score_1, robust_pca_score_1, mnp_score_1])
    average_score_2 = np.mean([uniform_score_2, optimized_score_2, weighted_rank_sum_score_2, inverse_variance_score_2, ahp_score_2, robust_pca_score_2, mnp_score_2])

    # SCORES =========================

    logger_common_symbol_length = 50
    new_line_char_lim = 50
    message1 = f"{folder_name_1} has the best chance replicating the reference!"
    message2 = f"{folder_name_2} has the best chance replicating the reference!"
    message3 = "Model determination is mixed, check individual scores, or the figures, to help you decide!"
    message4 = "No Model has a definitive outcome!"
    formatted_message1 = insert_newlines(message1, new_line_char_lim)
    formatted_message2 = insert_newlines(message2, new_line_char_lim)
    formatted_message3 = insert_newlines(message3, new_line_char_lim)
    formatted_message4 = insert_newlines(message4, new_line_char_lim)

    symbol_separator1 = "-"
    symbol_separator2 = "="

    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("Uniform Weights", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_1}: {round_and_format(uniform_score_1)}", file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_2}: {round_and_format(uniform_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("Optimized Weights", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_1}: {round_and_format(optimized_score_1)}", file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_2}: {round_and_format(optimized_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("Weighted Rank Sum-Based Weights", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_1}: {round_and_format(weighted_rank_sum_score_1)}", file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_2}: {round_and_format(weighted_rank_sum_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("Inverse Variance-Based Weights", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_1}: {round_and_format(inverse_variance_score_1)}", file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_2}: {round_and_format(inverse_variance_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("Analytic Hierarchy Process-Based Weights", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_1}: {round_and_format(ahp_score_1)}", file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_2}: {round_and_format(ahp_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("Robust Principal Component Analysis-Based Weights", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_1}: {round_and_format(robust_pca_score_1)}", file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_2}: {round_and_format(robust_pca_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("``Meat-N-Potatoes``-Based Weights", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_1}: {round_and_format(mnp_score_1)}", file_target=output_stats_file)
    log(f"General Accuracy Score for {folder_name_2}: {round_and_format(mnp_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log("Mean General Accuracy Scores", file_target=output_stats_file)
    log(symbol_separator1 * logger_common_symbol_length, file_target=output_stats_file)
    log(f"Mean General Accuracy Score for {folder_name_1}: {round_and_format(average_score_1)}", file_target=output_stats_file)
    log(f"Mean General Accuracy Score for {folder_name_2}: {round_and_format(average_score_2)}", file_target=output_stats_file)

    log("\n" + symbol_separator2 * logger_common_symbol_length, file_target=output_stats_file)
    log("Two-Model Direct Comparison", file_target=output_stats_file)
    log(symbol_separator2 * logger_common_symbol_length, file_target=output_stats_file)
    
    better_model_idx = model_idx_1 if (average_score_1 < average_score_2) else model_idx_2
    better_average_score = average_score_1 if (average_score_1 < average_score_2) else average_score_2
    
    log(f"{folder_name_1} won {wins_1}/7 methods", file_target=output_stats_file)
    log(f"{folder_name_2} won {wins_2}/7 methods", file_target=output_stats_file)
    log(f"{folder_name_1 if better_model_idx == model_idx_1 else folder_name_2} has the best MGAS of: {better_average_score:.4f}", file_target=output_stats_file)
    
    if average_score_1 < average_score_2 and wins_1 > wins_2:
        log("\n" + formatted_message1, file_target=output_stats_file)
    elif average_score_2 < average_score_1 and wins_2 > wins_1:
        log("\n" + formatted_message2, file_target=output_stats_file)
    elif ((average_score_1 < average_score_2) or (average_score_1 < average_score_2)) and wins_1 == wins_2:
        log("\n" + formatted_message3, file_target=output_stats_file)
    else:
        log("\n" + formatted_message4, file_target=output_stats_file)
    
    log("\n" + symbol_separator2 * logger_common_symbol_length, file_target=output_stats_file)
    log("Multi-Model Round-Robin Tournament", file_target=output_stats_file)
    log(symbol_separator2 * logger_common_symbol_length, file_target=output_stats_file)

    if len(model_dirs) < 3:
        log("Not enough models, please see Direct Comparison!", file_target=output_stats_file)
    else:
        try:
            comparison_results, win_counts = round_robin_comparisons(model_dirs)
            log("Round-robin tournament completed successfully.", level="DEBUG", file_target=output_stats_file)
            
            if len(model_dirs) > 3:
                # Use the updated report_top_winners function
                top_winner = report_top_winners(win_counts, top_n=3, logger=log, model_dirs=model_dirs)
                log("Top winners of the round-robin tournament identified.", level="DEBUG", file_target=output_stats_file)
            else:
                log("\nWin Counts for each model:", file_target=output_stats_file)
                for model_index, count in win_counts.items():
                    folder_name = os.path.basename(model_dirs[model_index])
                    log(f"{folder_name}: Wins = {count}", file_target=output_stats_file)
                top_winner = max(win_counts, key=win_counts.get)

            log(f"\n{os.path.basename(model_dirs[top_winner])} is the Tournament winner!", level="INFO", file_target=output_stats_file)

        except Exception as e:
            log(f"Round-robin comparison failed with error: {e}", level="CRITICAL", file_target=output_stats_file)
            log_debug(traceback.format_exc(), file_target=metric_log_file)

    # Ensure all logs are flushed to their respective files.
    # 
    log("Metrics and comparison results written to respective files.", level="INFO", file_target=metric_log_file)
    log("Summary comparison results updated.", level="INFO", file_target=metric_log_file)

# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [[[ MAIN ]]] XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

if __name__ == "__main__":
    main()